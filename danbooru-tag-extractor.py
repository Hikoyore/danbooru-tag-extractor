import requests
import re
from urllib.parse import urlparse

def clean_tag(tag):
    return tag.replace('_', ' ')

def extract_identifier(url):
    parsed = urlparse(url)
    domain = parsed.netloc
    path = parsed.path
    query = parsed.query

    if 'danbooru.donmai.us' in domain or 'aibooru.online' in domain:
        match = re.search(r'/posts/(\d+)', path)
        if match:
            return domain, match.group(1)
        match = re.search(r'/data/[^?]+\?(\d+)', url)
        if match:
            return domain, match.group(1)
        match = re.search(r'/([a-f0-9]{32})\.[a-z]+', url, re.IGNORECASE)
        if match:
            md5 = match.group(1)
            return domain, f"md5:{md5}"

    elif 'konachan.com' in domain or 'konachan.net' in domain:
        match = re.search(r'/post/show/(\d+)', path)
        if match:
            return domain, match.group(1)
        match = re.search(r'/posts/(\d+)', path)
        if match:
            return domain, match.group(1)
        match = re.search(r'/([a-f0-9]{32})\.[a-z]+', url, re.IGNORECASE)
        if match:
            md5 = match.group(1)
            return domain, f"md5:{md5}"
        match = re.search(r'[?&]post_id=(\d+)', query)
        if match:
            return domain, match.group(1)

    return None, None

def get_clean_tags_from_url(url):
    print("üîÑ –ü–æ–ª—É—á–∞—é —Ç–µ–≥–∏...")

    domain, identifier = extract_identifier(url)
    if not domain or not identifier:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ—Å—Ç–∞ –∏–∑ —Å—Å—ã–ª–∫–∏")
        return None

    if 'danbooru.donmai.us' in domain or 'aibooru.online' in domain:
        api_base = f"https://{domain}"
        endpoint = "/posts.json"
        tag_field = "tag_string"
    elif 'konachan.com' in domain or 'konachan.net' in domain:
        api_base = f"https://{domain}"
        endpoint = "/post.json"
        tag_field = "tags"
    else:
        print(f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        return None

    if identifier.startswith("md5:"):
        params = {"tags": identifier}
    else:
        params = {"tags": f"id:{identifier}"}

    api_url = api_base + endpoint

    try:
        response = requests.get(api_url, params=params)
        response.raise_for_status()
        data = response.json()

        if not isinstance(data, list) or len(data) == 0:
            print("‚ùå –ü–æ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None

        post = data[0]
        post_id = post.get('id', identifier)
        raw_tags = post.get(tag_field, "").split()
        if not raw_tags:
            print("‚ö†Ô∏è –£ –ø–æ—Å—Ç–∞ –Ω–µ—Ç —Ç–µ–≥–æ–≤")
            return None

        clean_tags = [clean_tag(tag) for tag in raw_tags]
        clean_tags.sort()

        domain_slug = domain.replace('.', '_')
        filename = f"tags_{domain_slug}_{post_id}_clean.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(", ".join(clean_tags))
            f.write(f"\n\n–í—Å–µ–≥–æ —Ç–µ–≥–æ–≤: {len(clean_tags)}")

        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(clean_tags)} —Ç–µ–≥–æ–≤ –≤ {filename}")
        return clean_tags

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
        return None
    except (KeyError, ValueError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

def main():
    print("=" * 50)
    print("Tag Extractor –¥–ª—è Danbooru, Aibooru, Konachan")
    print("=" * 50)
    print("–í—Å—Ç–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å—Ç–∞ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
    print("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å–∞–π—Ç—ã: danbooru.donmai.us, aibooru.online, konachan.com, konachan.net")
    print("–ü—Ä–∏–º–µ—Ä—ã:")
    print("  https://danbooru.donmai.us/posts/123456")
    print("  https://konachan.com/post/show/123456")
    print()
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ exit, quit –∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É")
    print()

    while True:
        url = input("üîó –°—Å—ã–ª–∫–∞: ").strip()
        if url.lower() in ['exit', 'quit', 'q', '']:
            print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
            break
        if url:
            get_clean_tags_from_url(url)
            print()

if __name__ == "__main__":
    main()
